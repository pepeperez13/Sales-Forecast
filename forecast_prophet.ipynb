{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7419024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "from prophet.plot import plot_plotly, plot_cross_validation_metric\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.io as pio\n",
    "from prophet.serialize import model_to_json, model_from_json\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose, STL\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf, q_stat\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "import shutil\n",
    "\n",
    "# Set default Plotly theme to have a white background\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "# Set default Matplotlib style to have a white background\n",
    "plt.style.use(\"default\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee81e5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate evaluation metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "    # mape = np.mean(np.abs((y_true - y_pred) / np.maximum(y_true, 1))) * 100\n",
    "    return rmse\n",
    "\n",
    "\n",
    "# Extract events from the calendar data\n",
    "def extract_holidays(calendar):\n",
    "    # Extract relevant columns and filter rows with non-null events\n",
    "    events = calendar[\n",
    "        (calendar[\"event_name_1\"].notna()) | (calendar[\"event_name_2\"].notna())\n",
    "    ][[\"date\", \"event_name_1\", \"event_name_2\"]]\n",
    "\n",
    "    # Reshape to long format\n",
    "    events_long = pd.melt(\n",
    "        events,\n",
    "        id_vars=[\"date\"],\n",
    "        value_vars=[\"event_name_1\", \"event_name_2\"],\n",
    "        var_name=\"event_type\",\n",
    "        value_name=\"event\",\n",
    "    )\n",
    "    events_long = events_long.dropna(subset=[\"event\"])  # Drop rows without events\n",
    "\n",
    "    # Rename columns to match Prophet's expectations\n",
    "    holidays = (\n",
    "        events_long.rename(columns={\"date\": \"ds\", \"event\": \"holiday\"})\n",
    "        .drop(columns=[\"event_type\"])\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "\n",
    "    return holidays\n",
    "\n",
    "\n",
    "# Define a function to transform and clean data\n",
    "def transform_and_clean_data(sales_data, start_date=\"2011-01-29\"):\n",
    "    sales_long = sales_data.melt(id_vars=[\"id\"], var_name=\"day_id\", value_name=\"sales\")\n",
    "    sales_long[\"day_num\"] = sales_long[\"day_id\"].str.extract(r\"(\\d+)\").astype(int)\n",
    "    sales_long[\"date\"] = pd.to_datetime(start_date) + pd.to_timedelta(\n",
    "        sales_long[\"day_num\"] - 1, unit=\"d\"\n",
    "    )\n",
    "    maxDate = sales_long[\"date\"].dt.strftime(\"%Y-%m-%d\").unique().max()\n",
    "    return sales_long[[\"id\", \"date\", \"sales\"]], maxDate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbd287d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "calendar = pd.read_csv(\"data/calendar_afcs2024.csv\")\n",
    "prices = pd.read_csv(\"data/sell_prices_afcs2024.csv\")\n",
    "sales_train = pd.read_csv(\"data/sales_train_validation_afcs2024.csv\")\n",
    "sales_validation = pd.read_csv(\"data/sales_test_validation_afcs2024.csv\")\n",
    "sales_test = pd.read_csv(\"data/sales_test_evaluation_afcs2024.csv\")\n",
    "\n",
    "\n",
    "# Transform training data\n",
    "cleaned_train_sales_data, last_date = transform_and_clean_data(\n",
    "    sales_train, start_date=\"2011-01-29\"\n",
    ")\n",
    "# display(cleaned_train_sales_data.head())\n",
    "# print(last_date)\n",
    "\n",
    "cleaned_validation_sales_data, last_date = transform_and_clean_data(\n",
    "    sales_validation, start_date=\"2011-01-29\"\n",
    ")\n",
    "# display(cleaned_validation_sales_data.head())\n",
    "# print(last_date)\n",
    "\n",
    "# # Prepare the test data\n",
    "cleaned_test_sales_data, last_day = transform_and_clean_data(\n",
    "    sales_test, start_date=\"2011-01-29\"\n",
    ")\n",
    "# display(cleaned_test_sales_data.head())\n",
    "# print(last_day)\n",
    "\n",
    "# Merge sales_validation and sales_train for training\n",
    "merged_train_data = pd.concat(\n",
    "    [cleaned_train_sales_data, cleaned_validation_sales_data],\n",
    ")\n",
    "display(merged_train_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9837a84",
   "metadata": {},
   "source": [
    "### SET THE NUMBER OF MODELS\n",
    "\n",
    "Using K or \"all\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53087930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique product IDs\n",
    "product_ids = merged_train_data[\"id\"].unique()\n",
    "\n",
    "# Calculate total sales for each product\n",
    "total_sales = (\n",
    "    merged_train_data.groupby(\"id\")[\"sales\"]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"sales\": \"total_sales\"})\n",
    ").sort_values(by=\"total_sales\", ascending=False)\n",
    "display(total_sales.head())\n",
    "\n",
    "\n",
    "# Get top K products based on total sales\n",
    "top_k = \"all\"\n",
    "\n",
    "\n",
    "if isinstance(top_k, int):\n",
    "    products = total_sales.nlargest(top_k, \"total_sales\")[\"id\"].tolist()\n",
    "elif top_k == \"all\":\n",
    "    products = product_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed72f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_with_exponential_smoothing(\n",
    "    train_data, test_data, product_dir, smoothing_level=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Forecast sales using Simple Exponential Smoothing and save the forecast plot.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a copy of the test data to store the forecasted values\n",
    "    forecast = test_data.copy()\n",
    "\n",
    "    ts = train_data[[\"ds\", \"y\"]].set_index(\"ds\")\n",
    "    # Fit the Simple Exponential Smoothing model\n",
    "    model = SimpleExpSmoothing(ts)  # Optimize smoothing level if not provided\n",
    "    model.fit(optimized=True, use_brute=True)\n",
    "    yhat = model.predict(\n",
    "        model.params, start=test_data[\"ds\"].min(), end=test_data[\"ds\"].max()\n",
    "    )\n",
    "    # Round the predictions to the nearest integer and ensure they are non-negative\n",
    "    yhat_rounded = np.clip(np.round(yhat), a_min=0, a_max=None)\n",
    "\n",
    "    # Assign the rounded predictions to the 'yhat' column in the forecast DataFrame\n",
    "    forecast[\"yhat\"] = yhat_rounded\n",
    "\n",
    "    # # Calculate metrics\n",
    "    mae = abs(forecast[\"sales\"] - forecast[\"yhat\"]).mean()\n",
    "    rmse = ((forecast[\"sales\"] - forecast[\"yhat\"]) ** 2).mean() ** 0.5\n",
    "\n",
    "    # Save forecast plot\n",
    "    tail_df = forecast.rename(columns={\"sales\": \"actual\", \"yhat\": \"predicted\"})\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.scatter(tail_df[\"ds\"], tail_df[\"actual\"], color=\"red\", label=\"Actual Test Data\")\n",
    "    ax.plot(\n",
    "        tail_df[\"ds\"],\n",
    "        tail_df[\"predicted\"],\n",
    "        color=\"blue\",\n",
    "        linestyle=\"--\",\n",
    "        label=\"Forecasted Sales\",\n",
    "    )\n",
    "    ax.legend()\n",
    "    ax.set_title(\"Simple Exponential Smoothing Forecast\")\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Sales\")\n",
    "    fig.savefig(product_dir / \"forecast_plot.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    return forecast, mae, rmse\n",
    "\n",
    "\n",
    "def forecast_with_moving_average(train_data, test_data, product_dir, window=28):\n",
    "    \"\"\"\n",
    "    Forecast sales using a moving average method dynamically over the test period\n",
    "    and save the forecast plot.\n",
    "    \"\"\"\n",
    "    # Combine train and test data for rolling mean calculation\n",
    "    combined = pd.concat(\n",
    "        [train_data[[\"ds\", \"y\"]], test_data[[\"ds\"]]], ignore_index=True\n",
    "    )\n",
    "    combined = combined.sort_values(by=\"ds\").reset_index(drop=True)\n",
    "\n",
    "    # Initialize the yhat list for predictions\n",
    "    yhat = []\n",
    "\n",
    "    # Dynamically calculate rolling mean for each test date\n",
    "    for i in range(len(train_data), len(combined)):\n",
    "        if i < window:  # Handle insufficient data for the rolling window\n",
    "            rolling_window = combined.iloc[:i][\"y\"].mean(\n",
    "                skipna=True\n",
    "            )  # Use available data\n",
    "        else:\n",
    "            rolling_window = combined.iloc[i - window : i][\"y\"].mean(skipna=True)\n",
    "\n",
    "        # Handle potential NaN by defaulting to 0\n",
    "        if pd.isna(rolling_window):\n",
    "            rolling_window = 0\n",
    "\n",
    "        yhat.append(round(rolling_window))  # Ensure integer predictions\n",
    "\n",
    "    # Assign the rolling forecast values to the test data\n",
    "    forecast = test_data.copy()\n",
    "    forecast[\"yhat\"] = yhat\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae = abs(forecast[\"sales\"] - forecast[\"yhat\"]).mean()\n",
    "    rmse = ((forecast[\"sales\"] - forecast[\"yhat\"]) ** 2).mean() ** 0.5\n",
    "\n",
    "    # Save forecast plot\n",
    "    tail_df = forecast.rename(columns={\"sales\": \"actual\", \"yhat\": \"predicted\"})\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.scatter(tail_df[\"ds\"], tail_df[\"actual\"], color=\"red\", label=\"Actual Test Data\")\n",
    "    ax.plot(\n",
    "        tail_df[\"ds\"],\n",
    "        tail_df[\"predicted\"],\n",
    "        color=\"blue\",\n",
    "        linestyle=\"--\",\n",
    "        label=\"Forecasted Sales\",\n",
    "    )\n",
    "    ax.legend()\n",
    "    ax.set_title(\"28-Day Moving Average Forecast\")\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Sales\")\n",
    "    fig.savefig(product_dir / \"forecast_plot.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    return forecast, mae, rmse\n",
    "\n",
    "\n",
    "# Define a function for Prophet forecasting\n",
    "def forecast_with_prophet(train_data, test_data, data_events, product_dir):\n",
    "    \"\"\"\n",
    "    Forecast sales using Prophet and save related plots and metrics.\n",
    "    \"\"\"\n",
    "    original_stdout = sys.stdout\n",
    "    original_stderr = sys.stderr\n",
    "\n",
    "    sys.stdout = open(os.devnull, \"w\")\n",
    "    sys.stderr = open(os.devnull, \"w\")\n",
    "\n",
    "    # Initialize and fit the Prophet model\n",
    "    model = Prophet(holidays=data_events)\n",
    "    model.add_country_holidays(country_name=\"US\")\n",
    "    model.add_seasonality(name=\"monthly\", period=30.5, fourier_order=5)\n",
    "    model.fit(train_data)\n",
    "\n",
    "    sys.stdout = original_stdout\n",
    "    sys.stderr = original_stderr\n",
    "\n",
    "    # Cross-validation\n",
    "    df_cv = cross_validation(model, horizon=\"28 days\", parallel=\"processes\")\n",
    "    df_p = performance_metrics(df_cv)\n",
    "    df_p.to_csv(product_dir / \"performance_metrics.csv\", index=False)\n",
    "\n",
    "    # Save components and cross-validation plots\n",
    "    fig = model.plot_components(model.predict(train_data))\n",
    "    fig.savefig(product_dir / \"components.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig = plot_cross_validation_metric(df_cv, metric=\"rmse\", rolling_window=0.1)\n",
    "    fig.savefig(product_dir / \"cross_validation.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Predict test data\n",
    "    forecast = model.predict(test_data)\n",
    "    forecast[\"yhat\"] = forecast[\"yhat\"].round().clip(lower=0).astype(int)\n",
    "    forecast = forecast.merge(test_data[[\"ds\", \"sales\"]], on=\"ds\", how=\"left\")\n",
    "\n",
    "    # Save forecast plot\n",
    "    tail_df = forecast.rename(columns={\"sales\": \"actual\", \"yhat\": \"predicted\"})\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.scatter(tail_df[\"ds\"], tail_df[\"actual\"], color=\"red\", label=\"Actual Test Data\")\n",
    "    ax.plot(\n",
    "        tail_df[\"ds\"],\n",
    "        tail_df[\"predicted\"],\n",
    "        color=\"blue\",\n",
    "        linestyle=\"--\",\n",
    "        label=\"Forecasted Sales\",\n",
    "    )\n",
    "    ax.legend()\n",
    "    fig.savefig(product_dir / \"forecast_plot.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Calculate RMSE and return\n",
    "    mae = abs(forecast[\"sales\"] - forecast[\"yhat\"]).mean()\n",
    "    rmse = ((forecast[\"sales\"] - forecast[\"yhat\"]) ** 2).mean() ** 0.5\n",
    "    return forecast, mae, rmse\n",
    "\n",
    "\n",
    "def process_product(product_id, avg_sales, threshold, days_to_analyze=365):\n",
    "    # Filter data for the specified product\n",
    "    train_data_filtered = merged_train_data[\n",
    "        merged_train_data[\"id\"] == product_id\n",
    "    ].rename(columns={\"date\": \"ds\", \"sales\": \"y\"})\n",
    "    print(f\"Train Data: {train_data_filtered.shape}\")\n",
    "\n",
    "    test_data_filtered = cleaned_test_sales_data[\n",
    "        cleaned_test_sales_data[\"id\"] == product_id\n",
    "    ].rename(columns={\"date\": \"ds\"})\n",
    "    print(f\"Test Data: {test_data_filtered.shape}\")\n",
    "\n",
    "    # Ensure the data is sorted by date\n",
    "    train_data_filtered = train_data_filtered.sort_values(by=\"ds\").reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "\n",
    "    # Convert 'ds' to datetime if not already\n",
    "    train_data_filtered[\"ds\"] = pd.to_datetime(train_data_filtered[\"ds\"])\n",
    "\n",
    "    # Filter the last 'days_to_analyze' days\n",
    "    end_date = train_data_filtered[\"ds\"].max()\n",
    "    start_date = end_date - pd.Timedelta(days=days_to_analyze)\n",
    "    recent_data = train_data_filtered[\n",
    "        (train_data_filtered[\"ds\"] > start_date)\n",
    "        & (train_data_filtered[\"ds\"] <= end_date)\n",
    "    ]\n",
    "\n",
    "    # Set the date as the index for decomposition\n",
    "    ts = recent_data.set_index(\"ds\")[\"y\"]\n",
    "\n",
    "    # Apply Box-Cox Transformation to stabilize variance\n",
    "    ts_transformed, lam = boxcox(ts + 1)  # Adding 1 to avoid log(0)\n",
    "\n",
    "    # Perform STL decomposition\n",
    "    stl = STL(ts_transformed, period=7, seasonal=7)\n",
    "    result = stl.fit()\n",
    "\n",
    "    # Save the decomposition plot\n",
    "    product_dir = Path(f\"products/{product_id}\")\n",
    "    product_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Perform additive decomposition\n",
    "    decomposition_week = seasonal_decompose(ts, model=\"additive\", period=7)\n",
    "\n",
    "    # Plot the decomposed components for additive model\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(12, 10), sharex=True)\n",
    "    axes[0].plot(ts, label=\"Observed\", color=\"black\")\n",
    "    axes[0].set_ylabel(\"Observed\")\n",
    "    axes[0].legend(loc=\"upper left\")\n",
    "    axes[1].plot(decomposition_week.trend, label=\"Trend\", color=\"blue\")\n",
    "    axes[1].set_ylabel(\"Trend\")\n",
    "    axes[1].legend(loc=\"upper left\")\n",
    "    axes[2].plot(decomposition_week.seasonal, label=\"Seasonal\", color=\"green\")\n",
    "    axes[2].set_ylabel(\"Seasonal\")\n",
    "    axes[2].legend(loc=\"upper left\")\n",
    "    axes[3].plot(decomposition_week.resid, label=\"Residual\", color=\"red\")\n",
    "    axes[3].set_ylabel(\"Residual\")\n",
    "    axes[3].legend(loc=\"upper left\")\n",
    "    axes[3].set_xlabel(\"Date\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        product_dir / f\"last_{days_to_analyze}_train_decomposition_additive_week.png\"\n",
    "    )\n",
    "\n",
    "    # Perform additive decomposition\n",
    "    decomposition_monthly = seasonal_decompose(ts, model=\"additive\", period=31)\n",
    "\n",
    "    # Plot the decomposed components for additive model\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(12, 10), sharex=True)\n",
    "    axes[0].plot(ts, label=\"Observed\", color=\"black\")\n",
    "    axes[0].set_ylabel(\"Observed\")\n",
    "    axes[0].legend(loc=\"upper left\")\n",
    "    axes[1].plot(decomposition_monthly.trend, label=\"Trend\", color=\"blue\")\n",
    "    axes[1].set_ylabel(\"Trend\")\n",
    "    axes[1].legend(loc=\"upper left\")\n",
    "    axes[2].plot(decomposition_monthly.seasonal, label=\"Seasonal\", color=\"green\")\n",
    "    axes[2].set_ylabel(\"Seasonal\")\n",
    "    axes[2].legend(loc=\"upper left\")\n",
    "    axes[3].plot(decomposition_monthly.resid, label=\"Residual\", color=\"red\")\n",
    "    axes[3].set_ylabel(\"Residual\")\n",
    "    axes[3].legend(loc=\"upper left\")\n",
    "    axes[3].set_xlabel(\"Date\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        product_dir / f\"last_{days_to_analyze}_train_decomposition_additive_monthly.png\"\n",
    "    )\n",
    "\n",
    "    # Convert trend and residual components to pandas Series to handle missing values\n",
    "    trend_series = pd.Series(result.trend, index=ts.index).dropna()\n",
    "    resid_series = pd.Series(result.resid, index=ts.index).dropna()\n",
    "\n",
    "    # Check for trend using Augmented Dickey-Fuller test\n",
    "    adf_result = adfuller(trend_series)\n",
    "    has_trend = (\n",
    "        adf_result[1] > 0.05\n",
    "    )  # p-value > 0.05 suggests non-stationarity (presence of trend)\n",
    "\n",
    "    # Check for seasonality by examining the seasonal component's variance\n",
    "    seasonal_variance = np.var(result.seasonal)\n",
    "    has_seasonality = (\n",
    "        seasonal_variance > 0.1\n",
    "    )  # Threshold can be adjusted based on domain knowledge\n",
    "\n",
    "    # Check for autocorrelation in residuals using Ljung-Box test\n",
    "    lb_test = q_stat(acf(resid_series, fft=False, nlags=10), len(resid_series))\n",
    "    lb_p_value = lb_test[1][-1]\n",
    "    has_autocorrelation = (\n",
    "        lb_p_value < 0.05\n",
    "    )  # p-value < 0.05 suggests significant autocorrelation\n",
    "\n",
    "    # Determine model complexity\n",
    "    if avg_sales >= threshold and (has_trend or has_seasonality or has_autocorrelation):\n",
    "        # Use Prophet method for complex patterns\n",
    "        forecast, mae, rmse = forecast_with_prophet(\n",
    "            train_data_filtered, test_data_filtered, data_events, product_dir\n",
    "        )\n",
    "    else:\n",
    "        # Use Exponential Smoothing method for simpler patterns\n",
    "        forecast, mae, rmse = forecast_with_exponential_smoothing(\n",
    "            train_data_filtered, test_data_filtered, product_dir\n",
    "        )\n",
    "\n",
    "    # Calculate additional metrics\n",
    "    test_weekly_avg = test_data_filtered[\"sales\"].mean()\n",
    "    test_total_avg = test_data_filtered[\"sales\"].sum()\n",
    "    return product_id, forecast, mae, rmse, test_weekly_avg, test_total_avg\n",
    "\n",
    "\n",
    "def calculate_metrics(forecast):\n",
    "    \"\"\"\n",
    "    Calculate evaluation metrics for the forecast.\n",
    "    \"\"\"\n",
    "    mae = abs(forecast[\"sales\"] - forecast[\"yhat\"]).mean()\n",
    "    rmse = ((forecast[\"sales\"] - forecast[\"yhat\"]) ** 2).mean() ** 0.5\n",
    "    weekly_avg = forecast[\"sales\"].mean()\n",
    "    total_avg = forecast[\"sales\"].sum()\n",
    "    return mae, rmse, weekly_avg, total_avg\n",
    "\n",
    "\n",
    "# Extract holidays from the calendar data\n",
    "data_events = extract_holidays(calendar)\n",
    "\n",
    "# Calculate thresholds\n",
    "avg_weekly_sales = merged_train_data.groupby(\"id\")[\"sales\"].sum() / (\n",
    "    len(merged_train_data[\"date\"].unique()) / 7\n",
    ")\n",
    "THRESHOLD_PERCENTILE = 0.4\n",
    "threshold = avg_weekly_sales.quantile(THRESHOLD_PERCENTILE)\n",
    "\n",
    "\n",
    "# Updated results collection to include all metrics\n",
    "results = [\n",
    "    process_product(pid, avg_weekly_sales[pid], threshold)\n",
    "    for pid in tqdm(products, desc=\"Processing Products\", leave=True)\n",
    "]\n",
    "\n",
    "# Save metrics and submission data\n",
    "output_dir = Path(\"products\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "metrics_data = [\n",
    "    {\n",
    "        \"Product-ID\": res[0],\n",
    "        \"MAE\": round(res[2], 2),  # Round to 2 decimals\n",
    "        \"RMSE\": round(res[3], 2),  # Round to 2 decimals\n",
    "        \"Test_Weekly_Average_Sales\": int(round(res[4])),  # Ensure integer for sales\n",
    "        \"Test_Total_Sales\": int(round(res[5])),  # Ensure integer for sales\n",
    "    }\n",
    "    for res in results\n",
    "]\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Save the metrics as a CSV file\n",
    "metrics_df.to_csv(output_dir / \"metrics.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc313d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the root directory and metrics file path\n",
    "products_dir = \"products/\"\n",
    "metrics_path = os.path.join(products_dir, \"metrics.csv\")\n",
    "\n",
    "# Read the metrics CSV\n",
    "metrics_df = pd.read_csv(metrics_path)\n",
    "\n",
    "# Initialize lists for SES and Prophet\n",
    "ses_ids = []\n",
    "prophet_ids = []\n",
    "\n",
    "# Collect product IDs based on directory suffixes\n",
    "for dir_name in os.listdir(products_dir):\n",
    "    if dir_name.endswith(\"_ses\"):\n",
    "        ses_ids.append(dir_name.replace(\"_ses\", \"\"))\n",
    "    elif dir_name.endswith(\"_prophet\"):\n",
    "        prophet_ids.append(dir_name.replace(\"_prophet\", \"\"))\n",
    "\n",
    "# Filter metrics for SES and Prophet\n",
    "ses_metrics = metrics_df[metrics_df[\"Product-ID\"].str.contains(\"|\".join(ses_ids))]\n",
    "prophet_metrics = metrics_df[\n",
    "    metrics_df[\"Product-ID\"].str.contains(\"|\".join(prophet_ids))\n",
    "]\n",
    "\n",
    "# Calculate averages for SES\n",
    "ses_avg_rmse = ses_metrics[\"RMSE\"].mean()\n",
    "ses_avg_weekly_sales = ses_metrics[\"Test_Weekly_Average_Sales\"].mean()\n",
    "ses_total_sales = ses_metrics[\"Test_Total_Sales\"].sum()\n",
    "\n",
    "# Calculate averages for Prophet\n",
    "prophet_avg_rmse = prophet_metrics[\"RMSE\"].mean()\n",
    "prophet_avg_weekly_sales = prophet_metrics[\"Test_Weekly_Average_Sales\"].mean()\n",
    "prophet_total_sales = prophet_metrics[\"Test_Total_Sales\"].sum()\n",
    "\n",
    "# Display the results\n",
    "print(\"SES Metrics:\")\n",
    "print(f\"  Average RMSE: {ses_avg_rmse:.2f}\")\n",
    "print(f\"  Average Weekly Sales: {ses_avg_weekly_sales:.2f}\")\n",
    "print(f\"  Total Sales: {ses_total_sales}\")\n",
    "\n",
    "print(\"\\nProphet Metrics:\")\n",
    "print(f\"  Average RMSE: {prophet_avg_rmse:.2f}\")\n",
    "print(f\"  Average Weekly Sales: {prophet_avg_weekly_sales:.2f}\")\n",
    "print(f\"  Total Sales: {prophet_total_sales}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8b1139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated metrics file saved to products/updated_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "def update_product_ids(metrics_path, products_dir, output_path):\n",
    "    \"\"\"\n",
    "    Updates the Product-ID column in the metrics.csv file based on whether\n",
    "    the product was forecasted using Prophet or SES.\n",
    "\n",
    "    Args:\n",
    "        metrics_path (str): Path to the metrics.csv file.\n",
    "        products_dir (str): Path to the products directory.\n",
    "        output_path (str): Path to save the updated metrics.csv file.\n",
    "    \"\"\"\n",
    "    # Load the metrics.csv file\n",
    "    metrics_df = pd.read_csv(metrics_path)\n",
    "\n",
    "    # Create a mapping for product ID suffixes based on the products directory\n",
    "    id_suffix_mapping = {}\n",
    "\n",
    "    # Iterate through the directories in the products folder\n",
    "    for dir_name in os.listdir(products_dir):\n",
    "        dir_path = os.path.join(products_dir, dir_name)\n",
    "\n",
    "        # Check if it's a directory and map the suffix\n",
    "        if os.path.isdir(dir_path):\n",
    "            if os.path.exists(os.path.join(dir_path, \"components.png\")):\n",
    "                id_suffix_mapping[dir_name] = \"_prophet\"\n",
    "            else:\n",
    "                id_suffix_mapping[dir_name] = \"_ses\"\n",
    "\n",
    "    # Function to update the Product-ID\n",
    "    def add_suffix(product_id):\n",
    "        # Extract the base ID by removing the \"_validation\" suffix\n",
    "        base_id = product_id.replace(\"_validation\", \"\")\n",
    "        # Determine the suffix from the mapping\n",
    "        suffix = id_suffix_mapping.get(base_id, \"\")\n",
    "        # Append the suffix and restore the \"_validation\"\n",
    "        return f\"{base_id}{suffix}_validation\" if suffix else product_id\n",
    "\n",
    "    # Apply the transformation to the Product-ID column\n",
    "    metrics_df[\"Product-ID\"] = metrics_df[\"Product-ID\"].apply(add_suffix)\n",
    "\n",
    "    # Save the updated metrics.csv file\n",
    "    metrics_df.to_csv(output_path, index=False)\n",
    "    print(f\"Updated metrics file saved to {output_path}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "metrics_path = \"products/metrics.csv\"\n",
    "products_dir = \"products/\"\n",
    "output_path = \"products/updated_metrics.csv\"\n",
    "update_product_ids(metrics_path, products_dir, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c30810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique product IDs from the training data\n",
    "product_ids = merged_train_data[\"id\"].unique()\n",
    "\n",
    "# Calculate total sales for each product and sort by descending total sales\n",
    "total_sales = (\n",
    "    merged_train_data.groupby(\"id\")[\"sales\"]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"sales\": \"total_sales\"})\n",
    "    .sort_values(by=\"total_sales\", ascending=False)\n",
    ")\n",
    "\n",
    "# Display the top products by total sales (preview)\n",
    "display(total_sales.head())\n",
    "\n",
    "# Define the number of top products to analyze (set to 20 or \"all\" for all products)\n",
    "top_k = 20\n",
    "\n",
    "# Retrieve product IDs for the top K products\n",
    "if isinstance(top_k, int):\n",
    "    top_products = total_sales.nlargest(top_k, \"total_sales\")[\"id\"].tolist()\n",
    "elif top_k == \"all\":\n",
    "    top_products = product_ids\n",
    "else:\n",
    "    raise ValueError(\"Invalid value for top_k. Must be an integer or 'all'.\")\n",
    "\n",
    "# Load the metrics CSV file\n",
    "metrics_path = \"products/metrics.csv\"  # Update to your actual file path\n",
    "metrics_df = pd.read_csv(metrics_path)\n",
    "\n",
    "# Ensure column names are standardized and check for expected column\n",
    "if \"Product-ID\" not in metrics_df.columns:\n",
    "    raise KeyError(\"Expected column 'Product-ID' not found in metrics.csv\")\n",
    "\n",
    "# Filter the metrics for the selected top products\n",
    "filtered_metrics = metrics_df[metrics_df[\"Product-ID\"].isin(top_products)]\n",
    "\n",
    "# Generate LaTeX table for the filtered metrics\n",
    "latex_table = filtered_metrics.to_latex(\n",
    "    index=False,  # Exclude the index column from the LaTeX table\n",
    "    header=True,  # Include column headers\n",
    "    float_format=\"%.2f\",  # Format float values to 2 decimal places\n",
    "    caption=\"Performance Metrics for Forecasting Models\",  # Add a caption\n",
    "    label=\"tab:performance_metrics\",  # Add a reference label\n",
    "    column_format=\"|l|c|c|c|c|\",  # Define column alignment\n",
    "    escape=True,  # Escape special LaTeX characters\n",
    "    longtable=True,  # Use longtable for tables spanning multiple pages\n",
    ")\n",
    "\n",
    "# Print the LaTeX table for inclusion in reports\n",
    "# print(latex_table)\n",
    "\n",
    "# Save filtered metrics to a CSV file\n",
    "output_path = \"filtered_metrics.csv\"  # Update to your desired output path\n",
    "filtered_metrics.to_csv(output_path, index=False)\n",
    "# print(f\"Filtered metrics saved to {output_path}\")\n",
    "\n",
    "# # Display the filtered metrics (optional)\n",
    "# display(filtered_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a333185d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Prophet Metrics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product-ID</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Test_Weekly_Average_Sales</th>\n",
       "      <th>Test_Total_Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>FOODS_3_141_TX_3_validation</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>FOODS_3_199_TX_3_validation</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>FOODS_3_128_TX_3_validation</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>FOODS_3_274_TX_3_validation</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>FOODS_3_194_TX_3_validation</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>FOODS_3_577_TX_3_validation</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>FOODS_3_050_TX_3_validation</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>FOODS_3_294_TX_3_validation</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>FOODS_3_045_TX_3_validation</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>FOODS_3_082_TX_3_validation</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Product-ID   MAE  RMSE  Test_Weekly_Average_Sales  \\\n",
       "92   FOODS_3_141_TX_3_validation  0.29  0.60                          0   \n",
       "119  FOODS_3_199_TX_3_validation  0.21  0.60                          0   \n",
       "82   FOODS_3_128_TX_3_validation  0.25  0.57                          0   \n",
       "166  FOODS_3_274_TX_3_validation  0.25  0.57                          0   \n",
       "117  FOODS_3_194_TX_3_validation  0.32  0.57                          0   \n",
       "344  FOODS_3_577_TX_3_validation  0.25  0.57                          0   \n",
       "27   FOODS_3_050_TX_3_validation  0.21  0.46                          0   \n",
       "178  FOODS_3_294_TX_3_validation  0.21  0.46                          0   \n",
       "25   FOODS_3_045_TX_3_validation  0.21  0.46                          0   \n",
       "49   FOODS_3_082_TX_3_validation  0.14  0.38                          0   \n",
       "\n",
       "     Test_Total_Sales  \n",
       "92                  5  \n",
       "119                 6  \n",
       "82                  5  \n",
       "166                 9  \n",
       "117                 6  \n",
       "344                 6  \n",
       "27                  2  \n",
       "178                 2  \n",
       "25                  6  \n",
       "49                  3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bottom 10 Prophet Metrics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product-ID</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Test_Weekly_Average_Sales</th>\n",
       "      <th>Test_Total_Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>FOODS_3_090_TX_3_validation</td>\n",
       "      <td>25.61</td>\n",
       "      <td>29.35</td>\n",
       "      <td>64</td>\n",
       "      <td>1799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>FOODS_3_444_TX_3_validation</td>\n",
       "      <td>21.46</td>\n",
       "      <td>23.20</td>\n",
       "      <td>23</td>\n",
       "      <td>653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>FOODS_3_120_TX_3_validation</td>\n",
       "      <td>20.14</td>\n",
       "      <td>22.67</td>\n",
       "      <td>15</td>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>FOODS_3_252_TX_3_validation</td>\n",
       "      <td>15.68</td>\n",
       "      <td>19.66</td>\n",
       "      <td>51</td>\n",
       "      <td>1440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>FOODS_3_580_TX_3_validation</td>\n",
       "      <td>7.50</td>\n",
       "      <td>19.20</td>\n",
       "      <td>9</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>FOODS_3_635_TX_3_validation</td>\n",
       "      <td>14.04</td>\n",
       "      <td>17.86</td>\n",
       "      <td>7</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>FOODS_3_586_TX_3_validation</td>\n",
       "      <td>13.96</td>\n",
       "      <td>16.25</td>\n",
       "      <td>73</td>\n",
       "      <td>2056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>FOODS_3_295_TX_3_validation</td>\n",
       "      <td>9.96</td>\n",
       "      <td>15.68</td>\n",
       "      <td>15</td>\n",
       "      <td>421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>FOODS_3_501_TX_3_validation</td>\n",
       "      <td>10.61</td>\n",
       "      <td>15.65</td>\n",
       "      <td>14</td>\n",
       "      <td>391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>FOODS_3_234_TX_3_validation</td>\n",
       "      <td>10.71</td>\n",
       "      <td>15.56</td>\n",
       "      <td>15</td>\n",
       "      <td>425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Product-ID    MAE   RMSE  Test_Weekly_Average_Sales  \\\n",
       "55   FOODS_3_090_TX_3_validation  25.61  29.35                         64   \n",
       "264  FOODS_3_444_TX_3_validation  21.46  23.20                         23   \n",
       "77   FOODS_3_120_TX_3_validation  20.14  22.67                         15   \n",
       "154  FOODS_3_252_TX_3_validation  15.68  19.66                         51   \n",
       "346  FOODS_3_580_TX_3_validation   7.50  19.20                          9   \n",
       "373  FOODS_3_635_TX_3_validation  14.04  17.86                          7   \n",
       "350  FOODS_3_586_TX_3_validation  13.96  16.25                         73   \n",
       "179  FOODS_3_295_TX_3_validation   9.96  15.68                         15   \n",
       "300  FOODS_3_501_TX_3_validation  10.61  15.65                         14   \n",
       "143  FOODS_3_234_TX_3_validation  10.71  15.56                         15   \n",
       "\n",
       "     Test_Total_Sales  \n",
       "55               1799  \n",
       "264               653  \n",
       "77                409  \n",
       "154              1440  \n",
       "346               241  \n",
       "373               188  \n",
       "350              2056  \n",
       "179               421  \n",
       "300               391  \n",
       "143               425  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 SES Metrics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product-ID</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Test_Weekly_Average_Sales</th>\n",
       "      <th>Test_Total_Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>FOODS_3_539_TX_3_validation</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>FOODS_3_328_TX_3_validation</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>FOODS_3_796_TX_3_validation</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>FOODS_3_793_TX_3_validation</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>FOODS_3_713_TX_3_validation</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>FOODS_3_758_TX_3_validation</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>FOODS_3_597_TX_3_validation</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>FOODS_3_779_TX_3_validation</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>FOODS_3_260_TX_3_validation</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>FOODS_3_553_TX_3_validation</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Product-ID   MAE  RMSE  Test_Weekly_Average_Sales  \\\n",
       "213  FOODS_3_539_TX_3_validation  0.04  0.19                          0   \n",
       "126  FOODS_3_328_TX_3_validation  0.04  0.19                          0   \n",
       "314  FOODS_3_796_TX_3_validation  0.04  0.19                          0   \n",
       "313  FOODS_3_793_TX_3_validation  0.04  0.19                          0   \n",
       "283  FOODS_3_713_TX_3_validation  0.04  0.19                          0   \n",
       "298  FOODS_3_758_TX_3_validation  0.04  0.19                          0   \n",
       "236  FOODS_3_597_TX_3_validation  0.04  0.19                          0   \n",
       "309  FOODS_3_779_TX_3_validation  0.04  0.19                          0   \n",
       "99   FOODS_3_260_TX_3_validation  0.04  0.19                          0   \n",
       "222  FOODS_3_553_TX_3_validation  0.04  0.19                          0   \n",
       "\n",
       "     Test_Total_Sales  \n",
       "213                 1  \n",
       "126                 1  \n",
       "314                 1  \n",
       "313                 1  \n",
       "283                 1  \n",
       "298                 1  \n",
       "236                 1  \n",
       "309                 1  \n",
       "99                  1  \n",
       "222                 1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bottom 10 SES Metrics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product-ID</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Test_Weekly_Average_Sales</th>\n",
       "      <th>Test_Total_Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>FOODS_3_147_TX_3_validation</td>\n",
       "      <td>2.64</td>\n",
       "      <td>4.47</td>\n",
       "      <td>3</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>FOODS_3_747_TX_3_validation</td>\n",
       "      <td>1.96</td>\n",
       "      <td>4.32</td>\n",
       "      <td>2</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>FOODS_3_047_TX_3_validation</td>\n",
       "      <td>1.96</td>\n",
       "      <td>2.32</td>\n",
       "      <td>2</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>FOODS_3_618_TX_3_validation</td>\n",
       "      <td>1.64</td>\n",
       "      <td>2.05</td>\n",
       "      <td>2</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>FOODS_3_278_TX_3_validation</td>\n",
       "      <td>1.64</td>\n",
       "      <td>1.96</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>FOODS_3_152_TX_3_validation</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.92</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>FOODS_3_827_TX_3_validation</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.83</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>FOODS_3_317_TX_3_validation</td>\n",
       "      <td>1.36</td>\n",
       "      <td>1.81</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>FOODS_3_545_TX_3_validation</td>\n",
       "      <td>1.39</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>FOODS_3_603_TX_3_validation</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Product-ID   MAE  RMSE  Test_Weekly_Average_Sales  \\\n",
       "50   FOODS_3_147_TX_3_validation  2.64  4.47                          3   \n",
       "294  FOODS_3_747_TX_3_validation  1.96  4.32                          2   \n",
       "20   FOODS_3_047_TX_3_validation  1.96  2.32                          2   \n",
       "247  FOODS_3_618_TX_3_validation  1.64  2.05                          2   \n",
       "107  FOODS_3_278_TX_3_validation  1.64  1.96                          2   \n",
       "52   FOODS_3_152_TX_3_validation  1.75  1.92                          1   \n",
       "328  FOODS_3_827_TX_3_validation  1.50  1.83                          2   \n",
       "122  FOODS_3_317_TX_3_validation  1.36  1.81                          1   \n",
       "217  FOODS_3_545_TX_3_validation  1.39  1.78                          2   \n",
       "241  FOODS_3_603_TX_3_validation  1.25  1.78                          2   \n",
       "\n",
       "     Test_Total_Sales  \n",
       "50                 74  \n",
       "294                59  \n",
       "20                 67  \n",
       "247                62  \n",
       "107                50  \n",
       "52                 25  \n",
       "328                42  \n",
       "122                26  \n",
       "217                51  \n",
       "241                49  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_top_and_bottom(input_file: str, sort_by: str = \"RMSE\", top_n: int = 10):\n",
    "    \"\"\"\n",
    "    Get the top N and bottom N rows from a metrics CSV file based on a specified column.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): Path to the input CSV file.\n",
    "        sort_by (str): The column name to sort by (default is \"RMSE\").\n",
    "        top_n (int): Number of rows to retrieve for the top and bottom (default is 10).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two DataFrames: (top N rows, bottom N rows).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the metrics CSV file\n",
    "        metrics_df = pd.read_csv(input_file)\n",
    "\n",
    "        # Check if the specified column exists\n",
    "        if sort_by not in metrics_df.columns:\n",
    "            raise ValueError(f\"Column '{sort_by}' not found in {input_file}\")\n",
    "\n",
    "        # Sort the DataFrame by the specified column\n",
    "        sorted_metrics = metrics_df.sort_values(by=sort_by, ascending=False)\n",
    "\n",
    "        # Get the top N and bottom N rows\n",
    "        top_rows = sorted_metrics.tail(top_n)\n",
    "        bottom_rows = sorted_metrics.head(top_n)\n",
    "\n",
    "        return pd.concat([top_rows, bottom_rows])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "# Example usage\n",
    "prophet = get_top_and_bottom(\"prophet_metrics.csv\")\n",
    "ses = get_top_and_bottom(\"ses_metrics.csv\")\n",
    "\n",
    "# Display the results\n",
    "print(\"Prophet Metrics:\")\n",
    "display(prophet)\n",
    "\n",
    "print(\"SES Metrics:\")\n",
    "display(ses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ffb1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_forecast_to_csv(results, output_dir):\n",
    "    \"\"\"\n",
    "    Save the forecasted values for each product in the required format.\n",
    "    Ensure all values are integers.\n",
    "    \"\"\"\n",
    "    forecast_data = []\n",
    "\n",
    "    for product_id, forecast, _, _, _, _ in results:\n",
    "        # Extract the forecasted values (yhat) as a list and ensure they are integers\n",
    "        forecast_values = forecast[\"yhat\"].astype(int).tolist()\n",
    "        # Create a row with the product_id and forecast values\n",
    "        forecast_row = [product_id] + forecast_values\n",
    "        forecast_data.append(forecast_row)\n",
    "\n",
    "    # Define the headers\n",
    "    headers = [\"id\"] + [f\"F{i}\" for i in range(1, 29)]  # Assuming a 28-day forecast\n",
    "\n",
    "    # Create a DataFrame\n",
    "    forecast_df = pd.DataFrame(forecast_data, columns=headers)\n",
    "\n",
    "    # Save to CSV\n",
    "    forecast_df.to_csv(output_dir / \"forecast_submission.csv\", index=False)\n",
    "\n",
    "\n",
    "# After processing products, save the forecast in the required format\n",
    "save_forecast_to_csv(results, output_dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
